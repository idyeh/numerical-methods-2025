{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"线性方程组\"\n",
        "---\n",
        "\n",
        "## 学习目标\n",
        "\n",
        "* **问题**：给定 $A\\in\\mathbb{R}^{n\\times n}$、$b\\in\\mathbb{R}^n$；求解 $Ax=b$。\n",
        "* **直接法**（一次分解，多次回代）\n",
        "\n",
        "  1. **高斯消元 ↔ LU 分解**；\n",
        "  2. **主元选取（部分/全主元）**提升稳定性；\n",
        "  3. **批量右端**：分解一次，复用解多个 $b$。\n",
        "* **误差与稳定性**：条件数 $\\kappa(A)$、残差 $\\|r\\|=\\|b-A\\hat x\\|$、**后向稳定性**；\n",
        "  $\\displaystyle \\frac{\\|\\hat x-x^*\\|}{\\|x^*\\|}\\lesssim \\kappa(A)\\cdot \\frac{\\|r\\|}{\\|b\\|}$（启发式）。\n",
        "* **迭代法**：Jacobi、Gauss–Seidel 的**收敛条件**（$\\rho(T)<1$、对角占优等）；**共轭梯度（CG）**用于 SPD；\n",
        "  **预条件**：Jacobi（对角）、IC/SSOR（概念） → 加速收敛。\n",
        "* **工程**：**vmap/批量 RHS**。\n",
        "\n",
        "\n",
        "# 一、直接法\n",
        "\n",
        "## 高斯消元\n",
        "\n",
        "取\n",
        "$$\n",
        "A=\\begin{bmatrix}\n",
        "2 & 1 & 1\\\\\n",
        "4 & -6 & 0\\\\\n",
        "-2 & 7 & 2\n",
        "\\end{bmatrix},\\qquad \\det A=-16\\neq 0.\n",
        "$$\n",
        "\n",
        "**Step 1（消去第1列下方）**：主元 $a_{11}=2$。乘子\n",
        "$$\n",
        "\\ell_{21}=\\frac{4}{2}=2,\\quad \\ell_{31}=\\frac{-2}{2}=-1.\n",
        "$$\n",
        "\n",
        "行变换\n",
        "$$\n",
        "R_2\\leftarrow R_2-\\ell_{21}R_1,\\quad R_3\\leftarrow R_3-\\ell_{31}R_1\n",
        "$$\n",
        "\n",
        "得\n",
        "$$\n",
        "A^{(1)}=\\begin{bmatrix}\n",
        "2 & 1 & 1\\\\\n",
        "0 & -8 & -2\\\\\n",
        "0 & 8 & 3\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "**Step 2（消去第2列下方）**：主元 $a^{(1)}_{22}=-8$。乘子\n",
        "$$\n",
        "\\ell_{32}=\\frac{8}{-8}=-1,\\quad R_3\\leftarrow R_3-\\ell_{32}R_2.\n",
        "$$\n",
        "\n",
        "得到上三角\n",
        "$$\n",
        "U=\\begin{bmatrix}\n",
        "2 & 1 & 1\\\\\n",
        "0 & -8 & -2\\\\\n",
        "0 & 0 & 1\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "把乘子收集到严格下三角并在对角补 1，得\n",
        "$$\n",
        "L=\\begin{bmatrix}\n",
        "1 & 0 & 0\\\\\n",
        "\\ell_{21} & 1 & 0\\\\\n",
        "\\ell_{31} & \\ell_{32} & 1\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "1 & 0 & 0\\\\\n",
        "2 & 1 & 0\\\\\n",
        "-1 & -1 & 1\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "可直接验证 $A=LU$。\n",
        "\n",
        "::: callout-note\n",
        "**求解 $Ax=b$**：先解 $Ly=Pb$（本例 $P=I$），再解 $Ux=y$。\n",
        ":::\n",
        "\n",
        "## 从消元到 LU\n",
        "\n",
        "* 在不换行的理想情形：存在下三角 $L$（对角为 1）与上三角 $U$，使 $A=LU$。\n",
        "* 通过消元系数 $l_{ik}=\\displaystyle\\frac{a_{ik}}{u_{kk}}$ 记录到 $L$，更新上三角为 $U$。\n",
        "\n",
        "**主元选取**\n",
        "\n",
        "* **部分主元**（列内最大绝对值交换到对角）：稳定常用，实现为 $PA=LU$。\n",
        "* **全主元**（行列同时交换）：更稳但代价高，通常不必。\n",
        "\n",
        "\n",
        "## 部分主元 LU"
      ],
      "id": "aa2b8739"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "def lu_factor_pp(A):\n",
        "    \"\"\"\n",
        "    LU with partial pivoting: PA = LU\n",
        "    返回 L（含 1 的对角）、U、piv（行交换记录）\n",
        "    \"\"\"\n",
        "    A = np.array(A, dtype=np.float64, copy=True)\n",
        "    n = A.shape[0]\n",
        "    piv = np.arange(n)\n",
        "    for k in range(n-1):\n",
        "        # 选列主元\n",
        "        i = k + np.argmax(np.abs(A[k:, k]))\n",
        "        if A[i, k] == 0:\n",
        "            raise np.linalg.LinAlgError(\"Singular matrix\")\n",
        "        if i != k:\n",
        "            A[[k, i], :] = A[[i, k], :]\n",
        "            piv[[k, i]] = piv[[i, k]]\n",
        "        # 消元\n",
        "        A[k+1:, k] /= A[k, k]\n",
        "        A[k+1:, k+1:] -= np.outer(A[k+1:, k], A[k, k+1:])\n",
        "    # 拆出 L, U\n",
        "    L = np.tril(A, -1) + np.eye(n)\n",
        "    U = np.triu(A)\n",
        "    return L, U, piv\n",
        "\n",
        "def lu_solve(L, U, piv, b):\n",
        "    \"\"\"\n",
        "    解 PAx = b\n",
        "    b 可为 (n,) 或 (n,m)（批量右端）\n",
        "    \"\"\"\n",
        "    b = np.array(b, dtype=np.float64, copy=False)\n",
        "    n = U.shape[0]\n",
        "    # 施加置换 P\n",
        "    if b.ndim == 1:\n",
        "        y = b[piv].copy()\n",
        "        # 前代：Ly = Pb\n",
        "        for i in range(n):\n",
        "            y[i] -= L[i, :i] @ y[:i]\n",
        "        # 回代：Ux = y\n",
        "        x = y.copy()\n",
        "        for i in range(n-1, -1, -1):\n",
        "            x[i] = (x[i] - U[i, i+1:] @ x[i+1:]) / U[i, i]\n",
        "        return x\n",
        "    else:\n",
        "        Y = b[piv, :].copy()\n",
        "        for i in range(n):\n",
        "            Y[i, :] -= L[i, :i] @ Y[:i, :]\n",
        "        X = Y.copy()\n",
        "        for i in range(n-1, -1, -1):\n",
        "            X[i, :] = (X[i, :] - U[i, i+1:] @ X[i+1:, :]) / U[i, i]\n",
        "        return X"
      ],
      "id": "88ad16ba",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**验证与对比**"
      ],
      "id": "7362a15f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 随机可逆矩阵与多 RHS\n",
        "rng = np.random.default_rng(0)\n",
        "A = rng.standard_normal((5, 5))\n",
        "A = A + 5 * np.eye(5)  # 稳一点\n",
        "B = rng.standard_normal((5, 3))\n",
        "L, U, piv = lu_factor_pp(A)\n",
        "X1 = lu_solve(L, U, piv, B)\n",
        "X2 = np.linalg.solve(A, B)\n",
        "print(\"||A X1 - B|| =\", np.linalg.norm(A @ X1 - B))\n",
        "print(\"||X1 - X2||  =\", np.linalg.norm(X1 - X2))"
      ],
      "id": "a5181569",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 主元：一个反例"
      ],
      "id": "050b8cee"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "eps = 1e-12\n",
        "A_bad = np.array([[eps, 1.0],[1.0, 1.0]], dtype=np.float64)\n",
        "b     = np.array([1.0, 2.0])\n",
        "# 无主元（手动实现时会爆精度） vs NumPy（内部含主元）\n",
        "x_np = np.linalg.solve(A_bad, b)\n",
        "print(\"cond(A_bad)≈\", np.linalg.cond(A_bad), \"  x_np =\", x_np)"
      ],
      "id": "a21f423a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "对于方程组\n",
        "$$\n",
        "\\begin{cases} \\epsilon x_1 + x_2 = 1 \\\\ x_1 + x_2 = 2 \\end{cases}\n",
        "$$\n",
        "\n",
        "作差得 $(1-\\epsilon)x_1 = 1$，所以 $x_1 = \\frac{1}{1-\\epsilon}$。\n",
        "\n",
        "代入第二个方程得 $x_2 = 2 - x_1 = 2 - \\frac{1}{1-\\epsilon} = \\frac{2(1-\\epsilon)-1}{1-\\epsilon} = \\frac{1-2\\epsilon}{1-\\epsilon}$。\n",
        "\n",
        "因为 $\\epsilon = 10^{-12}$ 极小，所以 $x_1 \\approx \\frac{1}{1} = 1$，$x_2 \\approx \\frac{1}{1} = 1$。\n",
        "NumPy 计算出的结果 $x_{\\text{np}}$ 应该非常接近 $[1.0, 1.0]$。\n",
        "\n",
        "::: callout-note\n",
        "不选主元可能导致“增长因子”极大，舍入误差被成倍放大。**部分主元**是工程默认。NumPy 的 `np.linalg.solve` 函数在底层调用了高度优化的线性代数库（如 LAPACK），这些库在实现高斯消元（或 LU 分解）时，默认使用了部分主元选择（Partial Pivoting）等技术。\n",
        ":::\n",
        "\n",
        "# 二、条件数、残差与稳定性\n",
        "\n",
        "## 条件数与残差不等式\n",
        "\n",
        "在数值线性代数中，对于求解线性方程组 $Ax = b$ 的一个近似解 $\\hat{x}$，我们通常关心它的误差 $e = \\hat{x} - x^*$ 有多大。其中真解 $x^*$ 满足 $Ax^*=b$。然而，我们通常无法直接计算误差 $e$，因为我们不知道精确解。定义**残差**\n",
        "$$\n",
        "r=b-A\\hat x.\n",
        "$$\n",
        "\n",
        "残差是衡量近似解 $\\hat{x}$ 代入原方程后，等式不平衡的程度。\n",
        "\n",
        "有\n",
        "$$\n",
        "A(\\hat x-x^*)=r\\ \\Rightarrow\\ \\hat x-x^* = A^{-1}r.\n",
        "$$\n",
        "\n",
        "于是\n",
        "$$\n",
        "\\frac{\\|\\hat x-x^*\\|}{\\|x^*\\|}\n",
        "\\le \\frac{\\|A^{-1}\\|\\,\\|r\\|}{\\|x^*\\|}.\n",
        "$$\n",
        "又因为 $\\|b\\|=\\|Ax^*\\|\\le \\|A\\|\\,\\|x^*\\|\\Rightarrow \\|x^*\\|\\ge \\frac{\\|b\\|}{\\|A\\|}$，\n",
        "\n",
        "代回得到\n",
        "$$\n",
        "\\boxed{\\quad\n",
        "\\frac{\\|e\\|}{\\|x^*\\|}\n",
        "\\ \\le\\ \\|A\\|\\,\\|A^{-1}\\| \\cdot \\frac{\\|r\\|}{\\|b\\|}\n",
        "\\ =\\ \\kappa(A)\\cdot \\frac{\\|r\\|}{\\|b\\|}.\n",
        "\\quad}\n",
        "$$\n",
        "\n",
        "如果矩阵 $A$ 是良态的（well-conditioned，即 $\\kappa(A)$ 较小），那么不等式很有用。它表明相对误差（$\\|e\\|/\\|x^*\\|$）和相对残差（$\\|r\\|/\\|b\\|$）是同数量级的。残差可以作为误差的良好**启发式指标**。\n",
        "\n",
        "如果矩阵 $A$ 是病态的（ill-conditioned，即 $\\kappa(A)$ 很大），那么不等式虽然在数学上是严格正确的，但它失去了实用价值：这个上界对实际的误差大小的指导性不强。我们知道误差不会超过这个界限，但它可能在界限内的任何地方，使得残差和误差之间的关系非常模糊。\n",
        "\n",
        "于是记为\n",
        "$$\n",
        "\\frac{\\|e\\|}{\\|x^*\\|}\\lesssim \\kappa(A)\\cdot \\frac{\\|r\\|}{\\|b\\|}\n",
        "$$\n",
        "\n",
        "::: callout-important\n",
        "残差小不一定意味着误差小。\n",
        ":::\n",
        "\n",
        "## 数值稳定性：**前向** vs **后向**\n",
        "\n",
        "- **前向稳定（forward stable）**：算法输出 $\\hat y$ 与真值 $y=f(x)$ 的**差**很小：\n",
        "  $$\n",
        "  \\frac{\\|\\hat y-y\\|}{\\|y\\|}=O(u),\n",
        "  $$\n",
        "\n",
        "  其中 $u$ 是机器精度。**注意**：对**病态问题**（$\\kappa$ 大），任何算法的前向误差都可能被放大。\n",
        "\n",
        "- **后向稳定（backward stable）**：算法输出 $\\hat y$ 可视为对**略微扰动**的输入 $\\tilde x=x+\\delta x$ 的精确值：\n",
        "  $$\n",
        "  \\hat y = f(\\tilde x),\\qquad \\frac{\\|\\delta x\\|}{\\|x\\|}=O(u).\n",
        "  $$\n",
        "\n",
        "  这意味着算法“**只把错误推回输入**”，问题若本身病态，前向误差仍会大，但算法本身是可信的。\n",
        "\n",
        "- **典型例子**  \n",
        "  - **高斯消元 + 部分主元（GEPP）**：对绝大多数矩阵是**后向稳定**的；可以解释为在 $(A+\\delta A)\\hat x=b$ 上**精确**，且 $\\|\\delta A\\|/\\|A\\|=O(u)$（常数依赖增长因子）。  \n",
        "  - **Horner 法**（多项式求值）是后向稳定的；  \n",
        "  - **天真相加**（同号大数与小数相加）可能**前向不稳定**（严重抵消）。\n",
        "\n",
        "## 实验：小残差≠解的误差小"
      ],
      "id": "c2b04fde"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "# 构造病态 SPD（调节 kappa 控制条件数）\n",
        "def make_spd_with_cond(n, kappa, seed=0):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    Q, _ = np.linalg.qr(rng.standard_normal((n,n)))\n",
        "    vals = np.logspace(0, np.log10(kappa), n)   # [1, kappa]\n",
        "    A = (Q * vals) @ Q.T\n",
        "    return (A + A.T)/2\n",
        "\n",
        "n = 40\n",
        "kappa = 1e10\n",
        "A = make_spd_with_cond(n, kappa)\n",
        "x_true = np.ones(n)\n",
        "b = A @ x_true\n",
        "\n",
        "# 用稳定的库解\n",
        "x_hat = np.linalg.solve(A, b)\n",
        "\n",
        "rel_res = np.linalg.norm(b - A@x_hat)/np.linalg.norm(b)\n",
        "rel_err = np.linalg.norm(x_hat - x_true)/np.linalg.norm(x_true)\n",
        "print(f\"κ(A)≈{np.linalg.cond(A):.2e}\")\n",
        "print(f\"relative residual   ≈ {rel_res:.2e}\")\n",
        "print(f\"relative solution error ≈ {rel_err:.2e}\")\n",
        "print(f\"κ(A) * rel_res      ≈ {np.linalg.cond(A)*rel_res:.2e}  (与上式同量级)\")"
      ],
      "id": "78af0a23",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "尽管**残差**达到 1e-16 量级，**解误差**仍可能在 1e-6 甚至更大；这正是 $\\kappa(A)$ 的放大效应。\n",
        "\n",
        "> 这不是算法“坏”，而是问题**病态**：**后向稳定**保证我们求解的是“稍微扰动过的 $A$”的精确解。\n",
        "\n",
        "::: callout-tip\n",
        "\n",
        "* 高斯消元的**乘子**就是 $L$ 的下三角元素；最终 $A=LU$（或 $PA=LU$）。\n",
        "* **部分主元**是抗增长因子、抗舍入误差。\n",
        "* 条件数给出残差到解误差的放大界：$\\text{forward} \\lesssim \\kappa \\times \\text{residual}$。\n",
        "* **后向稳定** ≠ **前向误差小**；它意味着“算法把错误推回输入”，问题若病态，前向误差依然大。\n",
        ":::\n",
        "\n",
        "## 比较残差与解误差"
      ],
      "id": "81e80d2a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams[\"font.sans-serif\"] = [\"PingFang SC\", \"Arial Unicode MS\"]\n",
        "plt.rcParams[\"axes.unicode_minus\"] = False\n",
        "\n",
        "\n",
        "def make_spd_with_cond(n, kappa):\n",
        "    # A = Q diag(λ) Q^T，λ 在 [1, kappa] 对数均匀\n",
        "    rng = np.random.default_rng(0)\n",
        "    M = rng.standard_normal((n, n))\n",
        "    Q, _ = np.linalg.qr(M)\n",
        "    vals = np.logspace(0, np.log10(kappa), n)\n",
        "    A = (Q * vals) @ Q.T  # 等价 Q diag(vals) Q^T\n",
        "    return (A + A.T) / 2\n",
        "\n",
        "\n",
        "def experiment(n=60, kappas=(1e1, 1e3, 1e6)):\n",
        "    rng = np.random.default_rng(1)\n",
        "    errs = []\n",
        "    ress = []\n",
        "    ks = []\n",
        "    for kappa in kappas:\n",
        "        A = make_spd_with_cond(n, kappa)\n",
        "        x_true = rng.standard_normal(n)\n",
        "        b = A @ x_true\n",
        "        x_hat = np.linalg.solve(A, b)\n",
        "        res = np.linalg.norm(b - A @ x_hat) / np.linalg.norm(b)\n",
        "        err = np.linalg.norm(x_hat - x_true) / np.linalg.norm(x_true)\n",
        "        ks.append(kappa)\n",
        "        ress.append(res)\n",
        "        errs.append(err)\n",
        "        print(f\"kappa≈{kappa:.1e}  rel_res={res:.2e}  rel_err={err:.2e}\")\n",
        "    plt.figure(figsize=(5.4, 3.4))\n",
        "    plt.loglog(ks, ress, \"-o\", label=\"relative residual\")\n",
        "    plt.loglog(ks, errs, \"-o\", label=\"relative solution error\")\n",
        "    plt.xlabel(\"target κ(A)\")\n",
        "    plt.grid(True, which=\"both\", alpha=0.3)\n",
        "    plt.legend()\n",
        "    plt.title(\"残差与解误差 vs 条件数（SPD示例）\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "experiment()"
      ],
      "id": "924a030e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 三、迭代法\n",
        "\n",
        "## 谱半径与收敛\n",
        "- **定义**：矩阵 $T\\in\\mathbb{R}^{n\\times n}$ 的**谱半径**（spectral radius）\n",
        "  $$\n",
        "  \\rho(T)=\\max\\{\\,|\\lambda|:\\lambda\\ \\text{是}\\ T\\ \\text{的特征值}\\,\\}.\n",
        "  $$\n",
        "- **基本事实**（Gelfand 公式）：\n",
        "  $$\n",
        "  \\rho(T)=\\lim_{k\\to\\infty}\\|T^k\\|^{1/k}\\quad(\\text{任一相容矩阵范数}).\n",
        "  $$\n",
        "- **收敛判据**（线性不动点迭代）  \n",
        "  考虑线性迭代\n",
        "  $$\n",
        "  x_{k+1}=T\\,x_k+c.\n",
        "  $$\n",
        "  若存在真解 $x^*$ 满足 $x^*=T x^*+c$，令误差 $e_k=x_k-x^*$，则\n",
        "  $$\n",
        "  e_{k+1}=T\\,e_k=T^{k+1}e_0.\n",
        "  $$\n",
        "  因此\n",
        "  $$\n",
        "  \\|e_k\\|\\le \\|T\\|^k\\,\\|e_0\\|,\\qquad \\text{若 }\\rho(T)<1\\ \\Rightarrow\\ T^k\\to 0.\n",
        "  $$\n",
        "  **定理**：对线性定系数迭代，$\\rho(T)<1$**当且仅当**对任意初值 $x_0$，$x_k\\to x^*$。\n",
        "\n",
        "> 证明思路：  \n",
        "> “$\\Leftarrow$” 若有某初值不收敛，则存在特征向量方向不衰减；  \n",
        "> “$\\Rightarrow$” 用 Neumann 级数 $(I-T)^{-1}=\\displaystyle\\sum_{j\\ge0}T^j$ 或 Gelfand 公式得 $T^k\\to 0$。\n",
        "\n",
        "\n",
        "## 一般迭代格式\n",
        "把 $A$ 拆分为\n",
        "$$\n",
        "A=M-N,\\qquad M\\ \\text{可逆}.\n",
        "$$\n",
        "把线性方程 $Ax=b$ 写作\n",
        "$$\n",
        "Mx=(N x)+b \\ \\Rightarrow\\ x=M^{-1}Nx+M^{-1}b.\n",
        "$$\n",
        "得到**一般的（stationary）迭代**：\n",
        "$$\n",
        "\\boxed{\\quad x_{k+1}=M^{-1}N\\,x_k+M^{-1}b.\\quad}\n",
        "$$\n",
        "\n",
        "- **迭代矩阵**：定义 $T=M^{-1}N$。  \n",
        "- **误差传播**：$e_{k+1}=T e_k$。  \n",
        "- **残差传播**（记 $r_k=b-Ax_k$）：\n",
        "  $$\n",
        "  r_{k+1}=b-Ax_{k+1}=(I-AM^{-1})\\,r_k.\n",
        "  $$\n",
        "\n",
        "- **收敛**：$\\rho(T)<1$（必要且充分）。\n",
        "\n",
        "\n",
        "> **把迭代看作“预条件 Richardson”**  \n",
        "> 也常写作\n",
        "> $$\n",
        "> x_{k+1}=x_k+M^{-1}(b-Ax_k)=x_k+M^{-1}r_k,\n",
        "> $$\n",
        "> 视 $M$ 为一种**预条件子**（近似 $A$ 且易解）。\n",
        "\n",
        "\n",
        "## 矩阵“拆分”\n",
        "\n",
        "记 $A=D+L+U$，其中\n",
        "\n",
        "- $D=\\mathrm{diag}(A)$ 对角阵\n",
        "- $L$ 严格下三角\n",
        "- $U$ 严格上三角\n",
        "\n",
        "均按**原符号**分解。\n",
        "\n",
        "1) **Jacobi**（每步只用旧解的其余分量）\n",
        "$$\n",
        "M=D,\\quad N=-(L+U)\\quad\\Rightarrow\\quad\n",
        "\\boxed{~x_{k+1}=D^{-1}\\bigl(b-(L+U)x_k\\bigr)~}\n",
        "$$\n",
        "\n",
        "迭代矩阵\n",
        "$$\n",
        "\\boxed{~T_J=-D^{-1}(L+U).~}\n",
        "$$\n",
        "\n",
        "2) **Gauss–Seidel（GS）**（用“最新”的下三角更新）\n",
        "$$\n",
        "M=D+L,\\quad N=-U\\quad\\Rightarrow\\quad\n",
        "\\boxed{~(D+L)\\,x_{k+1}=b-Ux_k~}\n",
        "$$\n",
        "\n",
        "显式写成分量形式（按行更新）即为经典 GS。\n",
        "\n",
        "> **收敛条件（常见充分条件）**  \n",
        ">\n",
        "> - 若 $A$ **严格对角占优**（按行或按列），则 Jacobi 与 GS 都收敛（$\\rho(T)<1$）。  \n",
        "> - 若 $A$ **对称正定（Symmetric Positive-Definite，SPD）**，则 **GS 收敛**；Jacobi 常也收敛但不保证最快。  \n",
        "> - 一般地，GS **通常收敛更快**（常见理论界：在某些条件下 $\\rho(T_{GS})\\le \\rho(T_J)^2$）。\n",
        "\n",
        "3) **SOR（超松弛）**  \n",
        "$(D+\\omega L)x_{k+1}=\\omega b+\\bigl[(1-\\omega)D-\\omega U\\bigr]x_k$，  \n",
        "$0<\\omega<2$ 时对 SPD 常收敛；$\\omega$ 可优化。\n",
        "\n",
        "\n",
        "## 误差界与停止准则\n",
        "\n",
        "- **误差界**：若选定相容范数 $\\|\\cdot\\|$ 使 $\\|T\\|\\le q<1$，则\n",
        "  $$\n",
        "  \\|e_k\\|\\le q^k\\|e_0\\|,\\qquad\n",
        "  \\|x_k-x^*\\|\\le \\frac{q}{1-q}\\,\\|x_k-x_{k-1}\\|.\n",
        "  $$\n",
        "- **常用停止准则**：\n",
        "  $$\n",
        "  \\frac{\\|r_k\\|}{\\|b\\|}\\le \\varepsilon,\\quad\\text{或}\\quad\n",
        "  \\frac{\\|x_k-x_{k-1}\\|}{\\|x_k\\|}\\le \\varepsilon.\n",
        "  $$\n",
        "  残差与误差成正比的常数取决于 $\\kappa(A)$：病态时残差小≠误差小。\n",
        "\n",
        "\n",
        "## Jacobi / GS 的分量公式\n",
        "\n",
        "**Jacobi**：\n",
        "\n",
        "第 $i$ 行\n",
        "$$\n",
        "a_{ii}x^{k+1}_i=b_i-\\sum_{j\\ne i}a_{ij}\\,x^k_j\n",
        "\\quad\\Rightarrow\\quad\n",
        "\\boxed{~x^{k+1}_i=\\frac{1}{a_{ii}}\\Bigl(b_i-\\sum_{j\\ne i}a_{ij}\\,x^k_j\\Bigr).~}\n",
        "$$\n",
        "\n",
        "**Gauss–Seidel**：\n",
        "$$\n",
        "a_{ii}x^{k+1}_i\n",
        "=b_i-\\sum_{j<i}a_{ij}\\,x^{k+1}_j-\\sum_{j>i}a_{ij}\\,x^k_j,\n",
        "$$\n",
        "$$\n",
        "\\boxed{~x^{k+1}_i=\\frac{1}{a_{ii}}\n",
        "\\Bigl(b_i-\\sum_{j<i}a_{ij}\\,x^{k+1}_j-\\sum_{j>i}a_{ij}\\,x^k_j\\Bigr).~}\n",
        "$$\n",
        "即：从 $i=1$ 到 $n$ 顺序更新，使用“刚算出的新值”。\n",
        "\n",
        "\n",
        "## 谱半径与收敛：算例\n",
        "\n",
        "取\n",
        "$$\n",
        "A=\\begin{bmatrix}\n",
        "4 & 1\\\\\n",
        "2 & 3\n",
        "\\end{bmatrix},\\quad b=\\begin{bmatrix}1\\\\1\\end{bmatrix},\\quad\n",
        "D=\\begin{bmatrix}4&0\\\\0&3\\end{bmatrix},\\\n",
        "L=\\begin{bmatrix}0&0\\\\2&0\\end{bmatrix},\\\n",
        "U=\\begin{bmatrix}0&1\\\\0&0\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "- **Jacobi**：$T_J=-D^{-1}(L+U)=\\begin{bmatrix}0&-1/4\\\\-2/3&0\\end{bmatrix}$。  \n",
        "  特征值 $\\lambda=\\pm\\sqrt{\\tfrac{1}{6}}$，$\\rho(T_J)=\\sqrt{1/6}\\approx 0.408<1$ ⇒ 收敛。\n",
        "\n",
        "- **GS**：$T_{GS}=-(D+L)^{-1}U$。  \n",
        "  $(D+L)^{-1}=\\begin{bmatrix}1/4&0\\\\-2/12&1/3\\end{bmatrix}$，\n",
        "  $T_{GS}=\\begin{bmatrix}0&-1/4\\\\0&1/6\\end{bmatrix}$，$\\rho(T_{GS})=1/6\\approx 0.167$ ⇒ **更快**。\n",
        "\n",
        "结论：$\\rho(T_{GS})<\\rho(T_J)$ ⇒ GS 比 Jacobi 收敛快。\n",
        "\n",
        "\n",
        "### 伪代码：**一般拆分迭代**与 Jacobi/GS 特例\n",
        "\n",
        "**一般迭代：$x_{k+1}=M^{-1}(N x_k+b)$**\n",
        "```\n",
        "procedure StationaryIter(A, b, split=(M, N), x0, tol, itmax):\n",
        "x ← x0\n",
        "for k = 0..itmax-1:\n",
        "x_new ← solve(M, N*x + b)     # 解 M x_new = N x + b\n",
        "if norm(b - A*x_new)/norm(b) < tol:\n",
        "return x_new, k+1\n",
        "x ← x_new\n",
        "return x, itmax\n",
        "\n",
        "```\n",
        "\n",
        "**Jacobi**（用 `solve(M,·)` 退化为“按对角元素相除”）\n",
        "```\n",
        "\n",
        "M ← diag(A);  N ← -(L+U)\n",
        "x_{k+1} ← (b - (L+U) x_k) ./ diag(A)\n",
        "\n",
        "```\n",
        "\n",
        "**Gauss–Seidel**（前代形式）\n",
        "```\n",
        "\n",
        "M ← D + L;  N ← -U\n",
        "解 (D+L) x_{k+1} = b - U x_k   # 逐行或三角前代\n",
        "\n",
        "```\n",
        "\n",
        "### 计算谱半径并验证收敛"
      ],
      "id": "f7dd7281"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "def spectral_radius(T):\n",
        "    lam = np.linalg.eigvals(T)\n",
        "    return np.max(np.abs(lam))\n",
        "\n",
        "def jacobi_matrix(A):\n",
        "    A = np.asarray(A, float)\n",
        "    D = np.diag(np.diag(A))\n",
        "    R = A - D\n",
        "    return -np.linalg.inv(D) @ R\n",
        "\n",
        "def gs_matrix(A):\n",
        "    A = np.asarray(A, float)\n",
        "    D = np.diag(np.diag(A))\n",
        "    L = np.tril(A, -1)\n",
        "    U = np.triu(A,  1)\n",
        "    return -np.linalg.inv(D+L) @ U\n",
        "\n",
        "A = np.array([[4.,1.],[2.,3.]])\n",
        "TJ = jacobi_matrix(A)\n",
        "TG = gs_matrix(A)\n",
        "print(\"rho(T_J) =\", spectral_radius(TJ))\n",
        "print(\"rho(T_GS) =\", spectral_radius(TG))"
      ],
      "id": "8acafa5b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: callout-note\n",
        "若 $\\rho(T)<1$ 则对任意 $x_0$ 收敛；可用上述 `T` 验证不同拆分的优劣。\n",
        ":::\n",
        "\n",
        "## 与 CG/PCG 的关系\n",
        "- Jacobi/GS/SOR **是定系数的“一般迭代”**；  \n",
        "- **CG/PCG** 是**非定系数（Krylov）迭代**，只适用于 **SPD**，但收敛更快、可用预条件（见本章后续与附录）。\n",
        "\n",
        "\n",
        "## 小结\n",
        "\n",
        "* **先看谱半径**：$\\rho(T)<1$ 是线性迭代收敛的标准。\n",
        "* **一般格式**来自 $A=M-N$，误差递推 $e_{k+1}=M^{-1}N\\,e_k$。\n",
        "* **Jacobi、GS** 是两种典型拆分，严格对角占优/SPD 情况下有收敛保证。\n",
        "* **收敛速度**：$|e_k|\\approx O(\\rho(T)^k)$，$\\rho$ 越小越快，GS 往往优于 Jacobi。\n",
        "* **停止准则**：残差/步长监控并结合 $\\kappa(A)$ 解读。\n",
        "\n",
        "\n",
        "## 完整迭代实现"
      ],
      "id": "aceb90bf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def jacobi(A, b, x0=None, tol=1e-10, itmax=10_000):\n",
        "    A = np.asarray(A, float); b = np.asarray(b, float)\n",
        "    n = b.size; x = np.zeros(n) if x0 is None else x0.copy()\n",
        "    D = np.diag(A); R = A - np.diagflat(D)\n",
        "    hist = []\n",
        "    for k in range(itmax):\n",
        "        x = (b - R @ x) / D\n",
        "        r = b - A @ x\n",
        "        hist.append(np.linalg.norm(r)/np.linalg.norm(b))\n",
        "        if hist[-1] < tol: break\n",
        "    return x, np.array(hist)\n",
        "\n",
        "def gauss_seidel(A, b, x0=None, tol=1e-10, itmax=10_000):\n",
        "    A = np.asarray(A, float); b = np.asarray(b, float)\n",
        "    n = b.size; x = np.zeros(n) if x0 is None else x0.copy()\n",
        "    hist = []\n",
        "    for k in range(itmax):\n",
        "        for i in range(n):\n",
        "            x[i] = (b[i] - (A[i,:i]@x[:i] + A[i,i+1:]@x[i+1:]))/A[i,i]\n",
        "        r = b - A @ x\n",
        "        hist.append(np.linalg.norm(r)/np.linalg.norm(b))\n",
        "        if hist[-1] < tol: break\n",
        "    return x, np.array(hist)"
      ],
      "id": "c8c0acd7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 四、共轭梯度法\n",
        "## 共轭梯度：从最优化出发\n",
        "\n",
        "考虑**对称正定（SPD）**的线性方程组\n",
        "\n",
        "$$\n",
        "Ax=b,\\qquad A=A^\\top\\succ0.\n",
        "$$\n",
        "\n",
        "等价地，最小化严格凸二次型\n",
        "$$\n",
        "\\min_{x\\in\\mathbb{R}^n}\\ \\phi(x)=\\tfrac12 x^\\top A x - b^\\top x.\n",
        "$$\n",
        "\n",
        "其梯度 $\\nabla\\phi(x)=Ax-b$，最优解满足 $Ax^*=b$。\n",
        "\n",
        "CG 只保证在 **SPD** 情况下正确、单调收敛。若 $A$ 非对称或不定，用 BiCG、GMRES、LSQR 等方法替代。\n",
        "\n",
        "\n",
        "## 最速下降\n",
        "\n",
        "**最速下降（Steepest Descent, SD）**每步沿负梯度方向做**一维最小化**\n",
        "$$\n",
        "x_{k+1}=x_k+\\alpha_k p_k\n",
        "$$\n",
        "其中\n",
        "$$\n",
        "p_k:=-r_k\n",
        "$$\n",
        "其中\n",
        "$$\n",
        "r_k:=b-Ax_k\n",
        "$$\n",
        "而\n",
        "$$\n",
        "\\alpha_k=\\arg\\min_{\\alpha}\\ \\phi(x_k+\\alpha p_k)\n",
        "       = \\frac{r_k^\\top r_k}{p_k^\\top A p_k}.\n",
        "$$\n",
        "\n",
        "SD 容易跑出“**锯齿**”且降速慢。\n",
        "\n",
        "## $A$-共轭及其性质\n",
        "\n",
        "CG 的关键是构造一组**两两 $A$-共轭（正交）**的方向\n",
        "$$\n",
        "p_i^\\top A p_j=0\\quad(i\\ne j),\n",
        "$$\n",
        "\n",
        "并在这些方向上做一维最小化。有性质：\n",
        "\n",
        "- 若 $\\{p_0,\\dots,p_{n-1}\\}$ 线性无关且两两 $A$-共轭，则在精确算术下 **最多 $n$ 步终止**。  \n",
        "- 第 $k$ 步的解 $x_k$ 是仿射空间 $x_0+\\mathcal{K}_k(A,r_0)$ 中 $\\phi$ 的最小点，其中\n",
        "$$\n",
        "\\mathcal{K}_k(A,r_0)=\\mathrm{span}\\{r_0,Ar_0,\\dots, A^{k-1}r_0\\}\n",
        "$$\n",
        "\n",
        "即 Krylov 子空间。\n",
        "\n",
        "## 递推推导 CG\n",
        "\n",
        "我们希望方向 $p_k$ 既像 SD（取 $p_0=-r_0$），又保持 $A$-共轭：\n",
        "$$\n",
        "p_{k+1} = -r_{k+1} + \\beta_k p_k.\n",
        "$$\n",
        "\n",
        "令 $\\alpha_k$ 仍为沿 $p_k$ 的线搜索最优步长：\n",
        "$$\n",
        "\\alpha_k=\\frac{r_k^\\top r_k}{p_k^\\top A p_k}.\n",
        "$$\n",
        "\n",
        "更新\n",
        "$$\n",
        "x_{k+1}=x_k+\\alpha_k p_k,\\quad r_{k+1}=r_k-\\alpha_k A p_k.\n",
        "$$\n",
        "\n",
        "为使 $p_{k+1}$ 与 $p_k$ $A$-共轭（正交），即 $p_{k+1}^\\top A p_k=0$，代入得\n",
        "$$\n",
        "(-r_{k+1}+\\beta_k p_k)^\\top A p_k=0\n",
        "\\ \\Rightarrow\\ \n",
        "\\beta_k=\\frac{r_{k+1}^\\top r_{k+1}}{r_k^\\top r_k}.\n",
        "$$\n",
        "\n",
        "这里使用了 $r_{k+1}^\\top A p_k = 0$，可由一维最小化一阶条件推出 $r_{k+1}\\perp p_k$。\n",
        "\n",
        "**由此得到标准 CG 公式：**\n",
        "$$\n",
        "\\boxed{\n",
        "\\begin{aligned}\n",
        "&\\text{初始化：}\\ x_0\\ \\text{给定},\\ r_0=b-Ax_0,\\ p_0=r_0.\\\\\n",
        "&\\text{for } k=0,1,2,\\dots\\\\\n",
        "&\\qquad \\alpha_k=\\frac{r_k^\\top r_k}{p_k^\\top A p_k},\\quad\n",
        "x_{k+1}=x_k+\\alpha_k p_k,\\quad\n",
        "r_{k+1}=r_k-\\alpha_k A p_k,\\\\\n",
        "&\\qquad \\beta_k=\\frac{r_{k+1}^\\top r_{k+1}}{r_k^\\top r_k},\\quad\n",
        "p_{k+1}=r_{k+1}+\\beta_k p_k.\n",
        "\\end{aligned}}\n",
        "$$\n",
        "\n",
        "- 只需 **一次 `matvec`（$A p_k$**）与向量内积 `axpy`，每步代价 $O(\\mathrm{nnz}(A))$。\n",
        "- 在精确算术下，$\\{p_k\\}$ $A$-共轭、$\\{r_k\\}$ 两两正交。\n",
        "\n",
        "\n",
        "## 收敛率与条件数\n",
        "在 $A$-范数 $\\|e\\|_A=\\sqrt{e^\\top A e}$ 下有经典界（Chebyshev 多项式论证）：\n",
        "$$\n",
        "\\boxed{\\quad\n",
        "\\frac{\\|x_k-x^*\\|_A}{\\|x_0-x^*\\|_A}\n",
        "\\ \\le\\ 2\\left(\\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1}\\right)^k,\\quad\n",
        "\\kappa:=\\kappa_2(A).\n",
        "\\quad}\n",
        "$$\n",
        "\n",
        "谱越**团簇**、$\\kappa$ 越小，收敛越快。\n",
        "\n",
        "::: callout-note\n",
        "当 $A$ **对称正定**时，定义\n",
        "\n",
        "$$\n",
        "\\quad \\|x\\|_A := \\sqrt{x^\\top A x}\n",
        "$$\n",
        "\n",
        "* 这是一个合法的范数，正定性来自 $A\\succ 0$）。\n",
        "* 在最小化二次型 $\\phi(x)=\\tfrac12 x^\\top A x - b^\\top x$ 的场景，$A$-范数有**能量意义**：\n",
        "\n",
        "  $$\n",
        "  \\|x-x^*\\|_A^2 \\;=\\; 2\\big(\\phi(x)-\\phi(x^*)\\big),\n",
        "  $$\n",
        "\n",
        "  因为 $Ax^*=b$。所以 CG 收敛等价于目标函数值的单调下降。\n",
        ":::\n",
        "\n",
        "## 预条件共轭梯度\n",
        "\n",
        "选 SPD 预条件子 $M\\approx A$，解等价的**左预条件系统**\n",
        "$$\n",
        "M^{-1}Ax=M^{-1}b.\n",
        "$$\n",
        "\n",
        "把**残差**经 $M^{-1}$ 预处理：\n",
        "$$\n",
        "z_k:=M^{-1} r_k\\quad(\\text{或解 } M z_k=r_k).\n",
        "$$\n",
        "\n",
        "预条件共轭梯度（PCG）的修改仅在内积权与 $\\beta$ 的定义：\n",
        "$$\n",
        "\\boxed{\n",
        "\\begin{aligned}\n",
        "&\\text{初始化：}\\ r_0=b-Ax_0,\\ z_0=M^{-1}r_0,\\ p_0=z_0.\\\\\n",
        "&\\text{for } k=0,1,2,\\dots\\\\\n",
        "&\\qquad \\alpha_k=\\frac{r_k^\\top z_k}{p_k^\\top A p_k},\\quad\n",
        "x_{k+1}=x_k+\\alpha_k p_k,\\quad\n",
        "r_{k+1}=r_k-\\alpha_k A p_k,\\\\\n",
        "&\\qquad z_{k+1}=M^{-1} r_{k+1},\\quad\n",
        "\\beta_k=\\frac{r_{k+1}^\\top z_{k+1}}{r_k^\\top z_k},\\quad\n",
        "p_{k+1}=z_{k+1}+\\beta_k p_k.\n",
        "\\end{aligned}}\n",
        "$$\n",
        "\n",
        "- 预条件把谱**压缩**、降低有效条件数，从而**显著加速**。  \n",
        "- 常见 $M$：对角（Jacobi）、IC(0)/ILU(k)、SSOR、几何/代数多重网格（作为外层预条件）等。  \n",
        "- **注意**：PCG 要求 $M$ 是 **SPD** 且 $M^{-1}$ 的应用要尽可能便宜。\n",
        "\n",
        "\n",
        "### 注意事项与实践建议\n",
        "- **SPD**：CG/PCG 依赖于 $A$-内积与共轭性质，非 SPD 会失效或不稳定。  \n",
        "- **舍入误差**会破坏正交性：可偶尔重置操作，即 $p_k:=r_k$（重启）或采用基于 Lanczos 的实现。  \n",
        "- **停止准则**：常用 $\\|r_k\\|/\\|b\\| \\le \\varepsilon$。病态问题上可监控 $\\|e_k\\|_A$ 的界或用真实误差可估计量。  \n",
        "- **预条件质量和成本的折中**：$M$ 太“好”可能预解太贵，总体迭代时间最小才是目标。  \n",
        "- **缩放/对称化**：简单的列/行缩放可改善数值性，右/左预条件需保持等价 SPD 结构。\n",
        "\n",
        "## SD/CG 在二维二次型上的步进“轨迹”图"
      ],
      "id": "5b77bc2f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 2D 二次型\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "A = np.array([[5.0, 2.0], [2.0, 1.5]])  # SPD\n",
        "b = np.array([1.0, 0.5])\n",
        "\n",
        "\n",
        "def phi(x):\n",
        "    return 0.5 * x @ A @ x - b @ x\n",
        "\n",
        "\n",
        "def grad(x):\n",
        "    return A @ x - b\n",
        "\n",
        "\n",
        "x_sd = np.array([1.5, -1.0])  # 初始点\n",
        "x_cg = x_sd.copy()\n",
        "\n",
        "# 采样等高线\n",
        "xx = np.linspace(-1.5, 1.8, 200)\n",
        "yy = np.linspace(-1.5, 1.5, 200)\n",
        "X, Y = np.meshgrid(xx, yy)\n",
        "Z = 0.5 * (A[0, 0] * X**2 + 2 * A[0, 1] * X * Y + A[1, 1] * Y**2) - (\n",
        "    b[0] * X + b[1] * Y\n",
        ")\n",
        "\n",
        "# 轨迹\n",
        "SD, CG = [x_sd.copy()], [x_cg.copy()]\n",
        "# SD 5 步\n",
        "for _ in range(5):\n",
        "    g = grad(x_sd)\n",
        "    p = -g\n",
        "    alpha = (g @ g) / (p @ (A @ p))\n",
        "    x_sd = x_sd + alpha * p\n",
        "    SD.append(x_sd.copy())\n",
        "# CG 2 步即可到达解（在 2D 精确算术）\n",
        "r = b - A @ x_cg\n",
        "p = r\n",
        "for _ in range(2):\n",
        "    Ap = A @ p\n",
        "    alpha = (r @ r) / (p @ Ap)\n",
        "    x_cg = x_cg + alpha * p\n",
        "    r_new = r - alpha * Ap\n",
        "    beta = (r_new @ r_new) / (r @ r)\n",
        "    p = r_new + beta * p\n",
        "    r = r_new\n",
        "    CG.append(x_cg.copy())\n",
        "\n",
        "plt.figure(figsize=(6, 4.8))\n",
        "cs = plt.contour(X, Y, Z, levels=20)\n",
        "plt.clabel(cs, inline=1, fontsize=8, fmt=\"%.1f\")\n",
        "SD = np.array(SD)\n",
        "CG = np.array(CG)\n",
        "plt.plot(SD[:, 0], SD[:, 1], \"-o\", label=\"Steepest Descent\")\n",
        "plt.plot(CG[:, 0], CG[:, 1], \"-o\", label=\"Conjugate Gradient\")\n",
        "plt.plot([b[0]], [b[1]], \"x\", ms=8, label=\"(not the minimizer)\")  # just distractor\n",
        "x_star = np.linalg.solve(A, b)\n",
        "plt.plot([x_star[0]], [x_star[1]], \"r*\", ms=12, label=\"x*\")\n",
        "plt.legend()\n",
        "plt.title(\"SD vs CG on a Quadratic\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "8a6fee61",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> 观察：SD “锯齿”缓慢逼近；CG 通过**互相 $A$-共轭**的方向快速“直冲”最优点。\n",
        "\n",
        "\n",
        "## 收敛曲线：PCG 加速示意（1D Poisson）"
      ],
      "id": "a76fa2d6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def cg(A, b, x0=None, tol=1e-10, itmax=None, M=None):\n",
        "    A = np.asarray(A, float)\n",
        "    b = np.asarray(b, float)\n",
        "    n = b.size\n",
        "    x = np.zeros(n) if x0 is None else x0.copy()\n",
        "    r = b - A @ x\n",
        "    if M is None:\n",
        "        z = r.copy()\n",
        "        applyM = lambda v: v\n",
        "    else:\n",
        "        D = np.diag(M) if M.ndim == 2 else M\n",
        "        applyM = lambda v: v / D\n",
        "        z = applyM(r)\n",
        "    p = z.copy()\n",
        "    rz_old = np.dot(r, z)\n",
        "    hist = [np.linalg.norm(r) / np.linalg.norm(b)]\n",
        "    if itmax is None:\n",
        "        itmax = 5 * n\n",
        "    for k in range(itmax):\n",
        "        Ap = A @ p\n",
        "        alpha = rz_old / np.dot(p, Ap)\n",
        "        x = x + alpha * p\n",
        "        r = r - alpha * Ap\n",
        "        res = np.linalg.norm(r) / np.linalg.norm(b)\n",
        "        hist.append(res)\n",
        "        if res < tol:\n",
        "            break\n",
        "        z = applyM(r)\n",
        "        rz_new = np.dot(r, z)\n",
        "        beta = rz_new / rz_old\n",
        "        p = z + beta * p\n",
        "        rz_old = rz_new\n",
        "    return x, np.array(hist)\n",
        "\n",
        "\n",
        "def poisson1d(n):\n",
        "    # A ~ tridiag(-1, 2, -1) ∈ R^{n×n}\n",
        "    e = np.ones(n)\n",
        "    A = np.diag(2 * e) - np.diag(e[:-1], 1) - np.diag(e[:-1], -1)\n",
        "    return A\n",
        "\n",
        "\n",
        "n = 200\n",
        "A = poisson1d(n)\n",
        "x_true = np.ones(n)\n",
        "b = A @ x_true\n",
        "\n",
        "xj, hj = jacobi(A, b, tol=1e-10)\n",
        "xg, hg = gauss_seidel(A, b, tol=1e-10)\n",
        "xc, hc = cg(A, b, tol=1e-10)\n",
        "xc2, hc2 = cg(A, b, tol=1e-10, M=np.diag(A))  # Jacobi 预条件\n",
        "\n",
        "plt.figure(figsize=(6.2, 3.6))\n",
        "plt.semilogy(hj, label=\"Jacobi\")\n",
        "plt.semilogy(hg, label=\"Gauss–Seidel\")\n",
        "plt.semilogy(hc, label=\"CG\")\n",
        "plt.semilogy(hc2, label=\"PCG (Jacobi)\")\n",
        "plt.xlabel(\"iteration\")\n",
        "plt.ylabel(\"relative residual\")\n",
        "plt.xlim(0, 120)\n",
        "plt.grid(True, which=\"both\", alpha=0.3)\n",
        "plt.legend()\n",
        "plt.title(\"迭代法收敛曲线（Poisson 1D）\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "36288c05",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> PCG 明显减少迭代步数，预条件压缩了谱（降低有效 $\\kappa$）。\n",
        "\n",
        "\n",
        "## 小结\n",
        "\n",
        "* CG = “Krylov 子空间最小化 $A$-范误差 + $A$-共轭方向 + 一维最小化”。\n",
        "* 只适用 **SPD**，非 SPD 需更换算法。\n",
        "* 收敛快慢 ≈ 谱的团簇程度与条件数，**预条件**是加速核心。\n",
        "* 实现要点：只需 `A@v` 与一次“预解” `solve(M,·)`，用向量内积与 `axpy` 组装，内存/带宽友好。\n",
        "\n",
        "\n",
        "# 课后作业\n",
        "\n",
        "## 回顾\n",
        "\n",
        "* **直接法**（中小规模/多 RHS）：一次 LU/Cholesky 分解，重复回代，很多数值库默认用部分主元法。\n",
        "* **条件数**决定从残差到解误差的放大倍率，因此**残差小 ≠ 解误差小**。\n",
        "* **迭代法**：Jacobi/GS 的 $\\rho(T)<1$；CG 适用于 **对称正定阵**，配合预条件可显著加速。\n",
        "* **批量 RHS**：向量化/并行可大幅提升吞吐。\n",
        "\n",
        "### a6-1（必做）：编程\n",
        "实现 **Cholesky** 分解（$A=LL^\\top$），并与 `np.linalg.cholesky` 做比较。测试在 SPD 与非 SPD 的情况下的程序行为。\n",
        "\n",
        "\n",
        "## 附录 A：批量右端与并行\n",
        "\n",
        "* 直接法：**分解一次**（LU/Cholesky），对 $B=[b_1,\\dots,b_m]$ 做两次三角解：$LY=PB$、$UX=Y$。\n",
        "* 迭代法：同一 $A$ 对不同 $b$ 可**共享矩阵–向量乘**。在矢量化/并行环境下，对多个 RHS 可**矩阵–矩阵乘**。"
      ],
      "id": "f315c836"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 复用上面的 lu_factor_pp / lu_solve\n",
        "rng = np.random.default_rng(42)\n",
        "A = poisson1d(400) + 1e-2 * np.eye(400)\n",
        "B = rng.standard_normal((400, 20))\n",
        "\n",
        "L, U, piv = lu_factor_pp(A)\n",
        "X_my = lu_solve(L, U, piv, B)\n",
        "X_np = np.linalg.solve(A, B)\n",
        "print(\"multi-RHS 误差:\", np.linalg.norm(X_my - X_np) / np.linalg.norm(X_np))"
      ],
      "id": "8fba2266",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 附录 B：JAX 版 CG、`vmap` 批量 RHS 与 `lax.scan` 时间推进\n"
      ],
      "id": "f4dff987"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "try:\n",
        "    import jax, jax.numpy as jnp\n",
        "    jax.config.update(\"jax_enable_x64\", True)\n",
        "\n",
        "    def cg_jax(A, b, x0=None, tol=1e-10, itmax=None, M_diag=None):\n",
        "        A = jnp.asarray(A); b = jnp.asarray(b)\n",
        "        n = b.size; \n",
        "        x = jnp.zeros(n) if x0 is None else jnp.asarray(x0)\n",
        "        if itmax is None: itmax = 5*n\n",
        "\n",
        "        def applyM(v):\n",
        "            return v if M_diag is None else v / M_diag\n",
        "\n",
        "        r0 = b - A @ x\n",
        "        z0 = applyM(r0); p0 = z0\n",
        "        rz0 = jnp.vdot(r0, z0).real\n",
        "        bnorm = jnp.linalg.norm(b)\n",
        "\n",
        "        def body(carry, _):\n",
        "            x, r, p, rz_old = carry\n",
        "            Ap = A @ p\n",
        "            alpha = rz_old / jnp.vdot(p, Ap).real\n",
        "            x_new = x + alpha * p\n",
        "            r_new = r - alpha * Ap\n",
        "            z_new = applyM(r_new)\n",
        "            rz_new = jnp.vdot(r_new, z_new).real\n",
        "            beta = rz_new / rz_old\n",
        "            p_new = z_new + beta * p\n",
        "            return (x_new, r_new, p_new, rz_new), jnp.linalg.norm(r_new)/bnorm\n",
        "\n",
        "        (x_fin, r_fin, _, _), hist = jax.lax.scan(body, (x, r0, p0, rz0), xs=None, length=itmax)\n",
        "        return x_fin, jnp.array(hist)\n",
        "\n",
        "    # vmap 批量 RHS\n",
        "    n = 200\n",
        "    A = jnp.array(poisson1d(n))\n",
        "    rng = np.random.default_rng(0)\n",
        "    B = jnp.array(rng.standard_normal((n, 8)))\n",
        "    Mdiag = jnp.diag(A)\n",
        "\n",
        "    solve_one = lambda b: cg_jax(A, b, tol=1e-10, itmax=500, M_diag=Mdiag)[0]\n",
        "    solve_many = jax.vmap(solve_one, in_axes=1, out_axes=1)  # 按列映射\n",
        "    X = solve_many(B)\n",
        "\n",
        "    print(\"JAX batch shapes:\", B.shape, X.shape)\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"JAX not available or skipped:\", e)"
      ],
      "id": "ce573aa7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**说明**\n",
        "\n",
        "* `lax.scan` 以时间步推进迭代，`vmap` 对列批量并行。\n",
        "* 在 CPU 环境上，JIT 后的稠密 `matvec`/批量计算通常比 Python 循环更快，对稀疏矩阵则需专门库。"
      ],
      "id": "351a657c"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/macops/edu/numerical-methods-2025/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}